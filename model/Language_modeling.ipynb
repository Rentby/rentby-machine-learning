{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information\n",
    "* Product Name\n",
    "* Rent Price\n",
    "* Product Photo\n",
    "* Renters\n",
    "* Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>rent_price</th>\n",
       "      <th>url_photo</th>\n",
       "      <th>link</th>\n",
       "      <th>renters</th>\n",
       "      <th>description</th>\n",
       "      <th>hiking</th>\n",
       "      <th>cosplay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aksesoris Fashion Topeng Cosplay Pesta Hallowe...</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>https://images.tokopedia.net/img/cache/200-squ...</td>\n",
       "      <td>https://ta.tokopedia.com/promo/v1/clicks/8a-xg...</td>\n",
       "      <td>LenkaWeddingShop</td>\n",
       "      <td>𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topeng Pesta Unisex Cosplay Aksesoris Party Pu...</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>https://images.tokopedia.net/img/cache/200-squ...</td>\n",
       "      <td>https://ta.tokopedia.com/promo/v1/clicks/8a-xg...</td>\n",
       "      <td>LenkaWeddingShop</td>\n",
       "      <td>𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aksesoris Pesta Unisex Topeng Cosplay Hallowee...</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>https://images.tokopedia.net/img/cache/200-squ...</td>\n",
       "      <td>https://ta.tokopedia.com/promo/v1/clicks/8a-xg...</td>\n",
       "      <td>LenkaWeddingShop</td>\n",
       "      <td>𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2pcs Telinga Elf Kuping Palsu Elf Ears Aksesor...</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>https://images.tokopedia.net/img/cache/200-squ...</td>\n",
       "      <td>https://ta.tokopedia.com/promo/v1/clicks/8a-xg...</td>\n",
       "      <td>ITOKOTOO</td>\n",
       "      <td>😄Selamat datang, Anda dapat menghubungi kami j...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ekor Kucing Cosplay Cat Cosplay Kawaii Cute Ha...</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>https://images.tokopedia.net/img/cache/200-squ...</td>\n",
       "      <td>https://ta.tokopedia.com/promo/v1/clicks/8a-xg...</td>\n",
       "      <td>Mireading</td>\n",
       "      <td>HI, welcome to our store - We provide various ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  rent_price  \\\n",
       "0  Aksesoris Fashion Topeng Cosplay Pesta Hallowe...      3650.0   \n",
       "1  Topeng Pesta Unisex Cosplay Aksesoris Party Pu...      3650.0   \n",
       "2  Aksesoris Pesta Unisex Topeng Cosplay Hallowee...      3200.0   \n",
       "3  2pcs Telinga Elf Kuping Palsu Elf Ears Aksesor...      1900.0   \n",
       "4  Ekor Kucing Cosplay Cat Cosplay Kawaii Cute Ha...      3500.0   \n",
       "\n",
       "                                           url_photo  \\\n",
       "0  https://images.tokopedia.net/img/cache/200-squ...   \n",
       "1  https://images.tokopedia.net/img/cache/200-squ...   \n",
       "2  https://images.tokopedia.net/img/cache/200-squ...   \n",
       "3  https://images.tokopedia.net/img/cache/200-squ...   \n",
       "4  https://images.tokopedia.net/img/cache/200-squ...   \n",
       "\n",
       "                                                link           renters  \\\n",
       "0  https://ta.tokopedia.com/promo/v1/clicks/8a-xg...  LenkaWeddingShop   \n",
       "1  https://ta.tokopedia.com/promo/v1/clicks/8a-xg...  LenkaWeddingShop   \n",
       "2  https://ta.tokopedia.com/promo/v1/clicks/8a-xg...  LenkaWeddingShop   \n",
       "3  https://ta.tokopedia.com/promo/v1/clicks/8a-xg...          ITOKOTOO   \n",
       "4  https://ta.tokopedia.com/promo/v1/clicks/8a-xg...         Mireading   \n",
       "\n",
       "                                         description  hiking  cosplay  \n",
       "0  𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...       0        1  \n",
       "1  𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...       0        1  \n",
       "2  𝐑𝐄𝐀𝐃𝐘 𝐒𝐓𝐎𝐊, 𝐋𝐀𝐍𝐆𝐒𝐔𝐍𝐆 𝐊𝐈𝐑𝐈𝐌 !!  𝐏𝐑𝐈𝐕𝐀𝐒𝐈 𝐃𝐈𝐉𝐀𝐌𝐈𝐍...       0        1  \n",
       "3  😄Selamat datang, Anda dapat menghubungi kami j...       0        1  \n",
       "4  HI, welcome to our store - We provide various ...       0        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/fathanah/cleaned_dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  5377\n",
      "Number of fields:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", dataset.shape[0])\n",
    "print(\"Number of fields: \", dataset.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display products names and preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Aksesoris Fashion Topeng Cosplay Pesta Hallowe...\n",
       "1       Topeng Pesta Unisex Cosplay Aksesoris Party Pu...\n",
       "2       Aksesoris Pesta Unisex Topeng Cosplay Hallowee...\n",
       "3       2pcs Telinga Elf Kuping Palsu Elf Ears Aksesor...\n",
       "4       Ekor Kucing Cosplay Cat Cosplay Kawaii Cute Ha...\n",
       "                              ...                        \n",
       "5372    Dunlopillo Hooded Thermal Blanket ( Selimut To...\n",
       "5373    TERMURAH KING RABBIT THERMAL BLANKET SELIMUT F...\n",
       "5374    NEW PRODUK KING RABBIT THERMAL BLANKET FLEECE ...\n",
       "5375    dunlopillo thermal & travel blanket black seli...\n",
       "5376    TaffSPORT Selimut Darurat Emergency Blanket Th...\n",
       "Name: product_name, Length: 5377, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['product_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unwanted characters and words in product name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['product_name'] = dataset['product_name'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "dataset['product_name'] = dataset['product_name'].apply(lambda x: x.replace('\\u200a',' '))\n",
    "dataset['product_name'] = dataset['product_name'].str.lower()\n",
    "corpus = dataset['product_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.91):\n",
    "            print(\"\\nDesired value is already achieved!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['product_name'])\n",
    "total_words = len (tokenizer.word_index)+1\n",
    "\n",
    "# n_gram sequence\n",
    "def n_gram_seqs(corpus, tokenizer):\n",
    "\n",
    "    input_sequences = []\n",
    "\n",
    "    for line in corpus:\n",
    "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "      for i in range(1,len(token_list)):\n",
    "        n_gram_sequences = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequences)\n",
    "    \n",
    "    return input_sequences\n",
    "\n",
    "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded corpus has shape: (47357, 39)\n"
     ]
    }
   ],
   "source": [
    "# function padded\n",
    "def pad_seqs(input_sequences, maxlen):\n",
    "    padded_sequences = pad_sequences(input_sequences, maxlen = maxlen)\n",
    "    return padded_sequences\n",
    "\n",
    "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
    "print(f\"padded corpus has shape: {input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature and label\n",
    "def features_and_labels(input_sequences, total_words):\n",
    "    features = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "    one_hot_labels = to_categorical(labels,num_classes = total_words)\n",
    "    return features, one_hot_labels\n",
    "features, labels = features_and_labels(input_sequences, total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(features, labels, callbacks=[myCallback()], epochs=30, verbose=1)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(total_words, max_sequence_len)\n",
    "model.fit(features, labels, callbacks=myCallback(), epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
    "\n",
    "model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format=keras {saved_model_path} ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip submission.zip *.bin model.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Reccomendation Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTENT_RECOMMENDATION\n",
    "\n",
    "# embedding input\n",
    "def get_embeddings(model, tokenizer, texts, max_sequence_len):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len-1)\n",
    "    embeddings = model.predict(padded_sequences)\n",
    "    return embeddings\n",
    "\n",
    "product_embeddings = get_embeddings(model, tokenizer, data['product_name'], max_sequence_len)\n",
    "\n",
    "# input comparison simmilarities\n",
    "def recommend_products(user_input, model, tokenizer, product_embeddings, data, max_sequence_len):\n",
    "    input_seq = tokenizer.texts_to_sequences([user_input])\n",
    "    input_padded = pad_sequences(input_seq, maxlen=max_sequence_len-1)\n",
    "    input_embedding = model.predict(input_padded)\n",
    "\n",
    "    similarities = cosine_similarity(input_embedding, product_embeddings)\n",
    "    similar_indices = similarities.argsort()[0][::-1]\n",
    "\n",
    "    recommendations = data.iloc[similar_indices][['product_name', 'rent_price', 'url_photo', 'link']]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulis = input(\"Masukkan Keyword: \")\n",
    "recommendations = recommend_products(tulis, model, tokenizer, product_embeddings, data, max_sequence_len)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting model accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.title('Training accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
